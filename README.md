Reddit Classification ProjectExecutive Summary1. Questions Presented* Predictors of General Reddit Popularity: What characteristics of a post on Reddit are most predictive of the overall interaction on a thread (as measured by number of comments)?* Predictors of Position on Abortion Debate: What use of vocabulary is particularly characteristic of a post on Reddit's pro-life politics subreddit?  What is particularly characteristic of a post on Reddit's pro-choice subreddit?  What vocabulary is most telling for discriminating between a pro-choice and a pro-life posting?* Predictors of Position on Evolution Debate: What use of vocabulary is particularly characteristic of a post on Reddit's pro-evolution politics subreddit?  What is particularly characteristic of a post on Reddit's pro-choice subreddit?  What vocabulary is most telling for discriminating between a pro-choice and a pro-life posting?2. Summary of MethodologyI gathered data to address these questions from a public server of archived Reddit comments and submissions (http://api.pushshift.io, documented in Notebook "01_Gather Reddit Content."  The data were samples of a maximum size of 500, sampled from evenly-spaced time slices over the last 1.5 years.  A. Predictors of General Reddit Popularity:For this part of the project, I only selected submissions that received a minimum qualifying number of 100 comments, in order to focus the inquiry on attention-grabbing Reddit posts.In order to evaluate the predictors of relative reddit popularity, I considered the title of a submission, the length of time since it was submitted, the number of words in the title, and the particular subreddit to which it was submitted.  Each submission was labeled based on whether the number of public comments on the submission was greater than or less than the median number of comments.  Thus, the classification problem was evenly divided between the 50% of recent Reddit posts that received fewer comments and the 50% percent of recent Reddit posts that received more comments.  B. Predictors of Position on Controversial IssuesTo evaluate the predictors of position on the abortion and evolution debates, I considered the full text of submissions and comments from the appropriate subreddits:* /r/prochoice* /r/Prolife* /r/DebateEvolution* /r/CreationFor this part of the project, I considered all submissions and comments, without filtering based on the number of comments.  I did, however, delete texts that were merely flags that the Redditt moderators had deleted a post, since I did not want the computer learning algorithm to fit based merely on the frequency with which particular subreddits are hit by trolls/spammers.  The class categories were approximately evenly divided between Prochoice and Prolife posts.  However, the class categories were somewhat unbalanced between Pro-evolution and Creationist posts (70% of documents were pro-evolution and 30% were pro-creationism).Textual data was analyzed based on bag-of-words vectorization, with common English stop-words removed. 3.  Key ResultsA. General Reddit Popularity:For sorting between highly-commented posts and less-commented posts, the baseline classification accuracy was 50% (i.e., the accuracy one could achieve simply by applying a uniform classification to all records).   A logistic regression model had only modest success in distinguishing between the high response and low response classes (i.e., 59% accuracy, F1 score = .54).  Changing the model to a tree-based classifier (Random Forest) did not improve performance (accuracy = 56%, F1 score = .54), nor did the NB Multinomial model (accuracy = 56%, F1 score = .53).The logistic regression model had a more difficult time correctly flagging the high response class (the positive result) than it had correctly flagging the low response class.  Model specificity was 65%, whereas model sensitivity was 53%.A review of the regression coefficients used by the logistic regression model provides some insight into the features that are most pertinent to a submission's popularity.  They key factor was the subreddit of posting.  The top subreddits for accumulating comments were:* "Funny" (odds of being in the high response class increased by 669%)* "Gifs" (odds increased by 405%)* "World News" (odds increased by 322%)* "League of Legends" (odds increased by 370%)The particular vocabulary used in the title of the Redditt post overall had a more modest impact.  The title vocabulary most likely to attract comment was:* The phrase "game thread" (odds increased by 209%)* The word "bad" (odds increased by 185%)* The word "kids" (odds increased by 156%)* The word "solo" (odds increased by 144%)The length of time elapsed since a submission was not a pertinent factor in classifying these data, but an important caveat is that I only considered submissions that already had at least 100 comments.  Time since submission might still be a pertinent factor in sorting less popular submissions.  The length of the title did not have a statistically significant association with the classification of a submission (p>.5, when regressed separately from the actual words in the title).  Surprisingly, the presence of question mark in the title strongly reduced the likelihood of receiving more than the median number of comments in response.  (7% as likely to be in the high comment class, p<.05, when regressed separately from textual features)B. Predictors of Position on Abortion DebateThe classification features were purely text-based, so I focused on applying the Naive Bayes Classifier, since it often gives good results in text classification tasks and it tends to gives easily interpretable results with bag-of-word modeling.  The baseline accuracy was 50.8%, since the classes (pro-life texts vs. pro-choice texts) were quite evenly balanced.  Naive Bayes achieved an accuracy of 70% with test data (F1 score = .69).  The model had a slightly harder time flagging pro-life posts (sensitivity was 67%) than correctly flagging pro-choice posts (specificity was 72%).  The model suffered from overfitting (it had a 98% accuracy with training data).The following 20 words and phrases had the highest conditional probability, given that the document was sourced from the pro-choice subreddit:'anti choice', 'get pregnant', 'fuck', 'troll', 'suicide', 'donation', 'viability', 'species', 'foster', 'fucking', 'effective', 'give birth', 'kidney', 'pill', 'op', 'pp', 'friend', 'flu', 'mistake', 'late term'The following 20 words and phrases had the highest conditional probability, given that the document was sourced from the pro-life subreddit:'bible', 'video', 'agreed', 'randy', 'abortion rights', 'war', 'pp', 'glad', 'shame', 'march', 'dont', 'oppose', 'terrible', 'life movement', 'democrats', 'beth', 'animals', 'church', 'christians', 'gay',Finally, the following 20 words and phrases were the most probative, meaning that there was the greatest absolute difference between the conditional probability of the word for the pro-life class and the conditional probability of the word for the pro-choice class:randydistinctively pro-lifebethdistinctively pro-lifesurrogacydistinctively pro-lifeanti choicersdistinctively pro-choiceanti choicedistinctively pro-choicepersonalitydistinctively pro-choicegay coupledistinctively pro-lifeheadlinedistinctively pro-lifefludistinctively pro-choicemarch lifedistinctively pro-lifeobjectivedistinctively pro-choicecharitydistinctively pro-lifequotesdistinctively pro-lifedissonancedistinctively pro-lifesickeningdistinctively pro-lifenprdistinctively pro-lifeabortion rightsdistinctively pro-lifeclimatedistinctively pro-lifefunctiondistinctively pro-choicesurrogatedistinctively pro-lifeC. Predictors of Position on Evolution DebateAs with the abortion debate newsgroups, I applied the Naive Bayes Classifier.  The baseline accuracy was 59.6%, reflecting that pro-evolution posts outnumbered creationist posts.  Naive Bayes achieved an accuracy of 70% with test data (F1 score was  .46).  The model had a substantially harder time flagging creationist posts (sensitivity was 32.2%) than correctly flagging pro-evolution posts (specificity was 95.2%).  The model also suffered from overfitting (accuracy on training data was 87%)The following 20 words and phrases had the highest conditional probability, given that the document was sourced from the pro-evolution subreddit:epistemology', 'biogenesis', 'troll', 'ribosomes', 'allele', 'virgin', 'muh', 'citation', 'tries', 'toe', 'unfalsifiable', 'ring', 'abiotic', 'chromosomes', 'holy', 'ribosomal', 'biodiversity', 'mendel', 'addressing', 'documentedThe following 20 words and phrases had the highest conditional probability, given that the document was sourced from the creationism subreddit:'morning', 'oec', 'omniscient', 'scripture', 'viewpoint', 'multiverse', 'private', 'confused', 'evening', 'online', 'sin', 'contamination', 'emotion', 'yep', 'ama', 'intelligently', 'fascinating', 'motivation', 'islam', 'particle',Finally, the following 20 words and phrases were the most probative, meaning that there was the greatest absolute difference between the conditional probability of seeing the word in the pro-evolution class and the conditional probability of seeing the word in the pro-creationism class:biogenesisdistinctively pro-evolutionvirgindistinctively pro-evolutionmuhdistinctively pro-evolutionepistemologydistinctively pro-evolutionoecdistinctively pro-creationismabioticdistinctively pro-evolutionribosomesdistinctively pro-evolutionmorningdistinctively pro-creationismtriesdistinctively pro-evolutionflagelladistinctively pro-evolutionomniscientdistinctively pro-creationismbiodiversitydistinctively pro-evolutionmotivationdistinctively pro-creationismringdistinctively pro-evolutionamadistinctively pro-creationismneccessarilydistinctively pro-creationismdenisovadistinctively pro-evolutionwolfdistinctively pro-creationismimmaterialdistinctively pro-evolutionbutterdistinctively pro-creationismeveningdistinctively pro-creationism4. Next StepsFurther work to identify interesting patterns of word usage between two polarized discussion groups could proceed by looking at larger data sets and exploring additional modeling strategies.  Another important next step is to evaluate the logistic regression coefficients for statistical significance.  4